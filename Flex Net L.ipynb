{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flex Net: A Configurable Neural Network for Sound Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avant Garde Composer\n",
    "\n",
    "Code to generate batches of random marimba compositions (with piano roll and corresponding audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sonic import *\n",
    "\n",
    "class Generate_Batch():\n",
    "    def __init__(self, batch_size, length_in_beats, sample_rate, bpm):\n",
    "        self.song = Song(SampleBank(), length_in_beats, bpm, sample_rate)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_inst_samples = self.song.num_inst_samples\n",
    "        self.length_in_beats = length_in_beats\n",
    "        self.bpm = bpm\n",
    "        self.sr = sample_rate\n",
    "        self.N = self.sr * self.length_in_beats * 60 // self.bpm\n",
    "        self.samples_per_beat = (self.sr * 60) // self.bpm\n",
    "        self.batch_audio = tf.Variable(tf.zeros([self.batch_size, self.length_in_beats-4, self.samples_per_beat*5], dtype=tf.float32))\n",
    "        self.batch_piano_roll = tf.Variable(tf.zeros([self.batch_size, self.length_in_beats-4, self.num_inst_samples], dtype=tf.float32))\n",
    "    \n",
    "    #@tf.function\n",
    "    def __chop__(self, song_output, song_notes_float):\n",
    "        audio = tf.TensorArray(tf.float32, size=(self.length_in_beats-4), element_shape=(self.samples_per_beat*5,))\n",
    "        piano_roll = tf.TensorArray(tf.float32, size=(self.length_in_beats-4), element_shape=(self.num_inst_samples,))\n",
    "        for j in tf.range(self.song.length_in_beats - 4):\n",
    "            audio.write(j, song_output[j*self.samples_per_beat:(j+5)*self.samples_per_beat])\n",
    "            piano_roll.write(j, tf.reshape(song_notes_float, [self.song.length_in_beats, self.song.num_inst_samples])[j])\n",
    "        return audio.stack(), piano_roll.stack()\n",
    "    \n",
    "    #@tf.function\n",
    "    def __call__(self):\n",
    "        for i in range(self.batch_size):\n",
    "            self.song.zero()\n",
    "            for j in range(np.random.choice([3,4,5,6,7,8,9,10,11,12])):\n",
    "                self.song.generate()\n",
    "            self.song.compile()\n",
    "            audio, piano_roll = self.__chop__(self.song.output, self.song.notes_float)\n",
    "            self.batch_audio[i].assign(audio)\n",
    "            self.batch_piano_roll[i].assign(piano_roll)\n",
    "        return self.batch_audio, self.batch_piano_roll    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "batches_per_epoch = 10\n",
    "\n",
    "generate_batch = Generate_Batch(batch_size, 4*10, 44100, 60*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example of a random composition\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "generate_batch.song.generate()\n",
    "example_composition = generate_batch.song.output.numpy()\n",
    "\n",
    "Audio(example_composition, rate=44100, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from neural import *\n",
    "\n",
    "class Network(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(Network, self).__init__(name=name)\n",
    "        self.layers = []\n",
    "        \n",
    "                \n",
    "        sample_rate = 44100                 # 44.1khz sample rate\n",
    "        frame_length = sample_rate // 10    # frame size of 0.1 seconds\n",
    "        num_frames = 5                      # five frames read at a time\n",
    "        input_features = int(frame_length * num_frames)\n",
    "    \n",
    "        self.layers.append(DisperseLayer(input_features, frame_length, 1.0))\n",
    "        num_chunks = num_frames\n",
    "        \n",
    "        self.layers.append(RFFTLayer(self.layers[-1].chunk_size))\n",
    "        \n",
    "        self.layers.append(SeluLayer(self.layers[-1].output_features, self.layers[-1].output_features // 16, scaling=1))\n",
    "        self.layers.append(SeluLayer(self.layers[-1].output_features, self.layers[-1].output_features // 2, scaling=1))\n",
    "        self.layers.append(SeluLayer(self.layers[-1].output_features, self.layers[-1].output_features // 2, scaling=1))\n",
    "    \n",
    "        \n",
    "        self.layers.append(JoinLayer(self.layers[-1].output_features, num_chunks))\n",
    "\n",
    "        self.layers.append(SeluLayer(self.layers[-1].output_features, self.layers[-1].output_features // 4, scaling=1))\n",
    "        self.layers.append(SigmoidLayer(self.layers[-1].output_features, 61, scaling=1))\n",
    "        \n",
    "        self.scalings = tuple([scaling for layer in self.layers for scaling in layer.scalings])\n",
    "\n",
    "    def save(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if not os.path.exists(os.path.join(path, f'layer_{i}')):\n",
    "                os.makedirs(os.path.join(path, f'layer_{i}'))\n",
    "            layer.save(os.path.join(path, f'layer_{i}'))\n",
    "            \n",
    "    def load(self, path):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.load(os.path.join(path, f'layer_{i}'))\n",
    "        \n",
    "    def apply(self, data, layers):\n",
    "        if len(layers) == 1:\n",
    "            return layers[0](data)\n",
    "        else:\n",
    "            return self.apply(layers[0](data), layers[1:])\n",
    "\n",
    "    #@tf.function\n",
    "    def __call__(self, data):\n",
    "        x = data\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        return x\n",
    "        #return self.apply(data, self.layers)\n",
    "        \n",
    "    def reset(self):\n",
    "        for layer in self.layers:\n",
    "            layer.reset()\n",
    "\n",
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_output(arr):\n",
    "    N = arr.shape[0]\n",
    "    M = arr.shape[1]\n",
    "    output = np.zeros([N, M])\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            output[i][j] = int(max(0, min(127, round(128 * arr[i][j]))))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output, Audio\n",
    "import seaborn as sns\n",
    "import sys, signal\n",
    "import matplotlib.pyplot as plt\n",
    "import plotting\n",
    "\n",
    "keep_going = True\n",
    "def signal_handler(signal, frame):\n",
    "    global keep_going\n",
    "    keep_going = False\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "#@tf.function\n",
    "def loss(trainable_variables, scalings, notes_actual, notes_predicted):\n",
    "    #note_correctness_error = 10000*tf.reduce_mean((notes_actual - notes_predicted) ** 2)\n",
    "    #note_correctness_error = 1 - tf.reduce_mean(tf.reduce_mean(((1/128) + notes_actual) * ((1/128) + notes_predicted), axis=-1) / (tf.reduce_mean(((1/128) + notes_predicted) ** 2, axis=-1) ** (1/2) * tf.reduce_mean(((1/128) + notes_actual) ** 2, axis=-1) ** (1/2)))\n",
    "    #energy_actual = 150*tf.reduce_mean(notes_actual ** 2, axis=-1) - (tf.reduce_mean(notes_actual, axis=-1) ** 2)\n",
    "    #energy_predicted = 150*tf.reduce_mean(notes_predicted ** 2, axis=-1) - (tf.reduce_mean(notes_predicted, axis=-1) ** 2)\n",
    "    #mean_energy_error = tf.reduce_mean(tf.abs(energy_actual - energy_predicted))\n",
    "    #amplitude = 10*tf.minimum(tf.maximum(1.0 - tf.reduce_max(notes_predicted), 0.0), 1.0)\n",
    "    #loss = (note_correctness_error + mean_energy_error)\n",
    "    note_cost = tf.reduce_mean(-notes_actual * tf.math.log(0.0001 + notes_predicted) - (1 - notes_actual) * tf.math.log(1.0001 - notes_predicted))\n",
    "    regularization_cost = 0.0\n",
    "    for i in range(len(trainable_variables)):\n",
    "        regularization_cost +=  tf.reduce_mean(-tf.math.minimum(trainable_variables[i], 0.0)) #* (1 / scalings[i])\n",
    "    cost = note_cost + regularization_cost\n",
    "    tf.print('Cost:', cost, 'Note Cost:', note_cost, 'Regularization Cost:', regularization_cost)\n",
    "    #tf.print('Loss', loss, 'Note Cosine Error', note_correctness_error, ', Energy Error', mean_energy_error)\n",
    "    #tf.print('loss', loss)#, 'note_correctness_error', note_correctness_error, 'mean_energy_error', mean_energy_error)\n",
    "    return cost\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def grad(network, input_pcms, notes_actual):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(network.trainable_variables)\n",
    "        notes_predicted = network(input_pcms)\n",
    "        loss_value = loss(network.trainable_variables, network.scalings, notes_actual, notes_predicted)\n",
    "    return loss_value, notes_predicted, tf.clip_by_global_norm(tape.gradient(loss_value, network.trainable_variables), 1000000.0)\n",
    "\n",
    "#@tf.function\n",
    "def apply_grads(trainable_vars, grads, scalings, learning_rate):\n",
    "    for i in range(len(grads)):\n",
    "        trainable_vars[i].assign_sub(learning_rate * (1 / scalings[i] ** 2) * grads[i])\n",
    "\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.001) #, momentum=0.01) #, momentum=0.1, nesterov=True)\n",
    "optimizer=tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "\n",
    "num_inst_samples = 61\n",
    "length_in_seconds = 4\n",
    "quantization = 10\n",
    "length_in_beats = length_in_seconds * quantization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "grads = tuple(tf.Variable(tf.zeros_like(var, dtype=tf.float32)) for var in network.trainable_variables)\n",
    "\n",
    "i = -1\n",
    "while keep_going:\n",
    "    i += 1\n",
    "    input_pcms, notes_actual = generate_batch()\n",
    "\n",
    "    loss_value, notes_predicted, (new_grads, global_norm) = grad(network, input_pcms, notes_actual)\n",
    "    print('Global Norm:', global_norm.numpy())\n",
    "    #if (2 ** -120) < global_norm.numpy() < 1.0:\n",
    "    #    scaling = 1.0 / global_norm.numpy()\n",
    "    #else:\n",
    "    #    scaling = 1.0    \n",
    "    for j in range(len(grads)):#[0,1]: #[4,5,6,7,8,9]:\n",
    "        grads[j].assign_add(tf.where(tf.math.is_nan(new_grads[j]), tf.zeros_like(new_grads[j]), new_grads[j]) / batches_per_epoch)\n",
    "\n",
    "    #apply_grads(network.trainable_variables, new_grads, learning_rate, scaling)\n",
    "    \n",
    "    if i % 100 == 99:\n",
    "        clear_output()\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        pass\n",
    "        fig = plotting.error(notes_predicted, notes_actual)\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "        del fig\n",
    "\n",
    "    \n",
    "    if (i % batches_per_epoch) == (batches_per_epoch - 1):\n",
    "        apply_grads(network.trainable_variables, grads, network.scalings, learning_rate)\n",
    "        #optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "    \n",
    "        for j in range(len(grads)):\n",
    "            grads[j].assign(tf.zeros_like(network.trainable_variables[j], dtype=tf.float32))\n",
    "\n",
    "        print(f'Epoch {i // batches_per_epoch} completed!')\n",
    "        \n",
    "    if i % 1000 == 999:\n",
    "        network.save('checkpoint4')\n",
    "        print('Network Parameters Saved!')\n",
    "\n",
    "    print('Step', i, 'complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network.save('checkpoint4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load('checkpoint3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### To run a single test example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotting\n",
    "\n",
    "input_pcms, notes_actual = generate_batch()\n",
    "\n",
    "loss_value, notes_predicted, (new_grads, global_norm) = grad(network, input_pcms, notes_actual)\n",
    "        \n",
    "plt = plotting.error(notes_predicted, notes_actual)\n",
    "\n",
    "del plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
