{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.examples.tutorials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8bf8ae5a5303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples.tutorials'"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "# declare the training data placeholders\n",
    "# input x - for 28 x 28 pixels = 784\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# now declare the output data placeholder - 10 digits\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000.0\n",
    "# A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].\n",
    "pcm = tf.compat.v1.placeholder(tf.float32, [None, None])\n",
    "\n",
    "# A 1024-point STFT with frames of 64 ms and 75% overlap.\n",
    "stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256,\n",
    "                       fft_length=1024)\n",
    "spectrograms = tf.abs(stfts)\n",
    "\n",
    "# Warp the linear scale spectrograms into the mel-scale.\n",
    "num_spectrogram_bins = stfts.shape[-1].value\n",
    "lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
    "linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "  num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n",
    "  upper_edge_hertz)\n",
    "mel_spectrograms = tf.tensordot(\n",
    "  spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\n",
    "  linear_to_mel_weight_matrix.shape[-1:]))\n",
    "\n",
    "# Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "\n",
    "# Compute MFCCs from log_mel_spectrograms and take the first 13.\n",
    "mfccs = tf.signal.mfccs_from_log_mel_spectrograms(\n",
    "  log_mel_spectrograms)[..., :13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfccs(pcm):\n",
    "    #sample_rate = 44100.0\n",
    "    # Input is two tensors of [batch_size, num_samples] mono PCM samples in the range [-1, 1].\n",
    "    #pcm_left = tf.compat.v1.placeholder(tf.float32, [None, int(sample_rate*60)])\n",
    "    # pcm_right = tf.compat.v1.placeholder(tf.float32, [None, int(sample_rate*60)])\n",
    "\n",
    "    # A 2048-point STFT with frames of ??? ms and 75% overlap.\n",
    "    stfts = tf.signal.stft(pcm, frame_length=2048, frame_step=256, fft_length=2048)\n",
    "    spectrograms = tf.abs(stfts)\n",
    "\n",
    "    # Warp the linear scale spectrograms into the mel-scale.\n",
    "    num_spectrogram_bins = stfts.shape[-1].value\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,\n",
    "        upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    # Compute MFCCs from log_mel_spectrograms and take the first 13.\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :13]\n",
    "    return mfccs\n",
    "\n",
    "def customLoss(yTrue,yPred):\n",
    "    yTrue_left = yTrue[:,0]\n",
    "    yTrue_right = yTrue[:,1]\n",
    "    yPred_left = yPred[:,0]\n",
    "    yPred_right = yPred[:,1]\n",
    "    mfccs_true_left = mfccs(yTrue_left)\n",
    "    mfccs_true_right = mfccs(yTrue_right)\n",
    "    mfcss_pred_left = mfcss(yPred_left)\n",
    "    mfcss_pred_right = mfcss(yPred_right)\n",
    "    return tf.mse(mfccs_true_left, mfccs_pred_left) + tf.mse(mfcss_true_right, mfcss_pred_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A simple Tensorflow 2 layer dense network example\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# load the sklearn breast cancer dataset\n",
    "bc = datasets.load_breast_cancer()\n",
    "X = bc.data[:, :]\n",
    "Y = bc.target\n",
    "\n",
    "# min max scale and binarize the target labels\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X,Y)\n",
    "label = LabelBinarizer()\n",
    "Y = label.fit_transform(Y)\n",
    "\n",
    "# train fraction\n",
    "frac = 0.9\n",
    "\n",
    "# shuffle dataset\n",
    "idx = np.random.randint(X.shape[0], size=len(X))\n",
    "X = X[idx]\n",
    "Y = Y[idx]\n",
    "\n",
    "train_stop = int(len(X) * frac)\n",
    "\n",
    "X_ = X[:train_stop]\n",
    "Y_ = Y[:train_stop]\n",
    "\n",
    "X_t = X[train_stop:]\n",
    "Y_t = Y[train_stop:]\n",
    "\n",
    "# plot the first 3 PCA dimensions of the sampled data\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "X_reduced = PCA(n_components=3).fit_transform(X_)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y_.ravel(),\n",
    "           cmap=plt.cm.Set1, edgecolor='k', s=40)\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"3rd eigenvector\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "plt.show()\n",
    "\n",
    "# create the TF neural net\n",
    "# some hyperparams\n",
    "training_epochs = 200\n",
    "\n",
    "n_neurons_in_h1 = 100\n",
    "n_neurons_in_h2 = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "#n_features = len(X[0])\n",
    "#labels_dim = 1\n",
    "\n",
    "sample_rate = 44100            # 44.1k samples per channel per second\n",
    "num_samples = 60*sample_rate   # 60 seconds of data\n",
    "num_channels = 2               # 2 channels\n",
    "\n",
    "num_pitches = 127\n",
    "num_instrument = 69\n",
    "\n",
    "n_features = (num_channels*num_samples) + (num_pitches+num_instruments)**2\n",
    "n_targets = num_channels*num_samples\n",
    "#############################################\n",
    "\n",
    "# basic 2 layer dense net (MLP) example adapted from\n",
    "# https://becominghuman.ai/creating-your-own-neural-network-using-tensorflow-fa8ca7cc4d0e\n",
    "\n",
    "# these placeholders serve as our input tensors\n",
    "x = tf.placeholder(tf.float32, [None, n_features], name='input')\n",
    "#t = tf.placeholder(tf.float32, [None, (69+127)*(69+127)], name='tone_transformation_tensor')\n",
    "y = tf.placeholder(tf.float32, [None, 2, n_targets], name='labels')\n",
    "\n",
    "# TF Variables are our neural net parameter tensors, we initialize them to random (gaussian) values in\n",
    "# Layer1. Variables are allowed to be persistent across training epochs and updatable bt TF operations\n",
    "W1 = tf.Variable(tf.truncated_normal([n_features, n_neurons_in_h1], mean=0, stddev=1 / np.sqrt(n_features)),\n",
    "                 name='weights1')\n",
    "b1 = tf.Variable(tf.truncated_normal([n_neurons_in_h1], mean=0, stddev=1 / np.sqrt(n_features)), name='biases1')\n",
    "\n",
    "# note the output tensor of the 1st layer is the activation applied to a\n",
    "# linear transform of the layer 1 parameter tensors\n",
    "# the matmul operation calculates the dot product between the tensors\n",
    "y1 = tf.nn.sigmoid((tf.matmul(x, W1) + b1), name='activationLayer1')\n",
    "\n",
    "# network parameters(weights and biases) are set and initialized (Layer2)\n",
    "W2 = tf.Variable(tf.random_normal([n_neurons_in_h1, n_neurons_in_h2], mean=0, stddev=1),\n",
    "                 name='weights2')\n",
    "b2 = tf.Variable(tf.random_normal([n_neurons_in_h2], mean=0, stddev=1), name='biases2')\n",
    "# activation function(sigmoid)\n",
    "y2 = tf.sigmoid((tf.matmul(y1, W2) + b2), name='activationLayer2')\n",
    "\n",
    "# output layer weights and biases\n",
    "Wo = tf.Variable(tf.random_normal([n_neurons_in_h2, n_targets], mean=0, stddev=1 ),\n",
    "                 name='weightsOut')\n",
    "bo = tf.Variable(tf.random_normal([n_targets], mean=0, stddev=1), name='biasesOut')\n",
    "\n",
    "# the sigmoid (binary softmax) activation is absorbed into TF's sigmoid_cross_entropy_with_logits loss\n",
    "#logits = (tf.matmul(y2, Wo) + bo)\n",
    "output = tf.sigmoid(tf.matmul(y2, Wo) + bo)\n",
    "#loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "loss = mfccs_loss\n",
    "\n",
    "# tap a separate output that applies softmax activation to the output layer\n",
    "# for training accuracy readout\n",
    "#a = tf.nn.sigmoid(logits, name='activationOutputLayer')\n",
    "\n",
    "# optimizer used to compute gradient of loss and apply the parameter updates.\n",
    "# the train_step object returned is ran by a TF Session to train the net\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# prediction accuracy\n",
    "# compare predicted value from network with the expected value/target\n",
    "\n",
    "#correct_prediction = tf.equal(tf.round(a), y)\n",
    "# accuracy determination\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\")\n",
    "\n",
    "#############################################\n",
    "# ***NOTE global_variables_initializer() must be called before creating a tf.Session()!***\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# create a session for training and feedforward (prediction). Sessions are TF's way to run\n",
    "# feed data to placeholders and variables, obtain outputs and update neural net parameters\n",
    "with tf.Session() as sess:\n",
    "    # ***initialization of all variables... NOTE this must be done before running any further sessions!***\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # training loop over the number of epochs\n",
    "    batch_size = 50\n",
    "    batches = int(len(X_) / batch_size)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        losses = 0\n",
    "        accs = 0\n",
    "        for j in range(batches):\n",
    "            idx = np.random.randint(X_.shape[0], size=batch_size)\n",
    "            X_b = X_[idx]\n",
    "            Y_b = Y_[idx]\n",
    "\n",
    "            # train the network, note the dictionary of inputs and labels\n",
    "            sess.run(train_step, feed_dict={x: X_b, y: Y_b})\n",
    "            # feedforwad the same data and labels, but grab the accuracy and loss as outputs\n",
    "            acc, l, soft_max_a = sess.run([accuracy, loss, a], feed_dict={x: X_b, y: Y_b})\n",
    "\n",
    "            losses = losses + np.sum(l)\n",
    "            accs = accs + np.sum(acc)\n",
    "        print(\"Epoch %.8d \" % epoch, \"avg train loss over\", batches, \" batches \", \"%.4f\" % (losses/batches),\n",
    "              \"avg train acc \", \"%.4f\" % (accs/batches))\n",
    "\n",
    "        # test on the holdout set\n",
    "        acc, l, soft_max_a = sess.run([accuracy, loss, a], feed_dict={x: X_t, y: Y_t})\n",
    "        print(\"Epoch %.8d \" % epoch, \"test loss %.4f\" % np.sum(l),\n",
    "              \"test acc %.4f\" % acc)\n",
    "\n",
    "print(soft_max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfccs(pcm):\n",
    "\n",
    "    # sample_rate = 44100            # 44.1k samples per channel per second\n",
    "    # num_samples = 60*sample_rate   # 60 seconds of data\n",
    "    # num_channels = 2               # 2 channels\n",
    "    # # Input is Tensor of [batch_size, num_samples, num_channels] PCM samples in the range [-1, 1].\n",
    "    # pcm = tf.compat.v1.placeholder(tf.float32, [None, num_channels, num_samples])\n",
    "\n",
    "    # A 2048-point STFT with frames of ??? ms and 75% overlap.\n",
    "    stfts = tf.signal.stft(pcm, frame_length=2048, frame_step=256, fft_length=2048)\n",
    "    spectrograms = tf.abs(stfts)\n",
    "\n",
    "    # Warp the linear scale spectrograms into the mel-scale.\n",
    "    num_spectrogram_bins = stfts.shape[-1].value\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins,\n",
    "                                                                        num_spectrogram_bins,\n",
    "                                                                        sample_rate,\n",
    "                                                                        lower_edge_hertz,\n",
    "                                                                        upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    # Compute MFCCs from log_mel_spectrograms and take the first 13.\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :13]\n",
    "\n",
    "def mfccs_loss(pcm_true, pcm_pred):\n",
    "    mfccs_true = mfccs_loss(pcm_true)\n",
    "    mfccs_pred = mfccs_loss(pcm_pred)\n",
    "    return tf.losses.mean_squared_error(mfccs_true, mfccs_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
