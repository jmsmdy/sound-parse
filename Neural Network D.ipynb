{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bass_arco_ff_sulE', 'Cello_arco_ff_sulG', 'AltoFlute_vib_ff', 'Bassoon_ff', 'Viola_arco_ff_sulA', 'Violin_pizz_ff_sulE', 'Xylophone_rosewood_roll_ff', 'Cello_pizz_ff_sulD', 'AltoSax_NoVib_ff', 'Cello_arco_ff_sulA', 'BassFlute_ff', 'Trumpet_vib_ff', 'SopSax_vib_ff', 'TenorTrombone_ff', 'Bass_arco_ff_sulD', 'Viola_pizz_ff_sulC', 'Bass_pizz_ff_sulE', 'Cello_arco_ff_sulC', 'Trumpet_novib_ff', 'Vibraphone_sustain_ff', 'bells_plastic_ff', 'Bass_arco_ff_sulA', 'Bass_pizz_ff_sulC', 'AltoSax_vib_ff', 'Cello_pizz_ff_sulG', 'Marimba_deadstroke_ff', 'Xylophone_hardrubber_ff', 'Xylophone_hardrubber_roll_ff', 'Marimba_cord_ff', 'Vibraphone_shortsustain_ff', 'Viola_arco_ff_sulG', 'Violin_arco_ff_sulE', 'Cello_arco_ff_sulD', 'Violin_pizz_ff_sulD', 'Bass_pizz_ff_sulG', 'BbClarinet_ff', 'Crotale_ff', 'Marimba_rubber_ff', 'Flute_nonvib_ff', 'Bass_pizz_ff_sulD', 'Violin_pizz_ff_sulA', 'Bass_arco_ff_sulC', 'Flute_vib_ff', 'Violin_arco_ff_sulA', 'Viola_pizz_ff_sulG', 'BassClarinet_ff', 'Viola_arco_ff_sulC', 'Vibraphone_dampen_ff', 'Cello_pizz_ff_sulC', 'Horn_ff', 'Viola_pizz_ff_sulA', 'bells_brass_ff', 'BassTrombone_ff', 'Viola_pizz_ff_sulD', 'Xylophone_rosewood_ff', 'Bass_arco_ff_sulG', 'Violin_pizz_ff_sulG', 'Tuba_ff', 'Viola_arco_ff_sulD', 'Bass_pizz_ff_sulA', 'Cello_pizz_ff_sulA', 'Marimba_yarn_ff', 'Oboe_ff', 'Violin_arco_ff_sulG', 'Marimba_roll_ff', 'Violin_arco_ff_sulD', 'Vibraphone_bow', 'EbClarinet_ff', 'SopSax_nonvib_ff'}\n"
     ]
    }
   ],
   "source": [
    "from os import listdir, getcwd, rename, remove\n",
    "from os.path import isfile, join\n",
    "\n",
    "path = join(getcwd(), 'preprocessed_samples')\n",
    "\n",
    "files = [f for f in listdir(path) if isfile(join(path, f)) and f[0] != '.']\n",
    "\n",
    "samples = []\n",
    "for f in files:\n",
    "    sample = {'instrument' : '_'.join(f.split('_')[:-1]),\n",
    "              'midi_number' : int(f.split('_')[-1][:-4]),\n",
    "              'filename' : join(path, f)}\n",
    "    samples.append(sample)\n",
    "\n",
    "samples = sorted(samples, key=lambda x: (x['instrument'], x['midi_number']))\n",
    "    \n",
    "instrument_names = set([s['instrument'] for s in samples])\n",
    "print(instrument_names)\n",
    "instruments = {}\n",
    "for inst in instrument_names:\n",
    "    samples_for_inst = {s['midi_number'] : s['filename'] for s in samples if s['instrument'] == inst}\n",
    "    instruments[inst] = {\n",
    "        'samples' : samples_for_inst,\n",
    "        'min_note' : min(samples_for_inst.keys()),\n",
    "        'max_note' : max(samples_for_inst.keys())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_instruments = ['AltoSax_NoVib_ff',\n",
    "                          'BassClarinet_ff',\n",
    "                          'BassFlute_ff',\n",
    "                          'BassTrombone_ff',\n",
    "                          'Bass_arco_ff_sulA',\n",
    "                          'Bass_arco_ff_sulD',\n",
    "                          'Bass_arco_ff_sulE',\n",
    "                          'Bass_arco_ff_sulG',\n",
    "                          'Bass_pizz_ff_sulA',\n",
    "                          'Bass_pizz_ff_sulD',\n",
    "                          'Bass_pizz_ff_sulE',\n",
    "                          'Bass_pizz_ff_sulG',\n",
    "                          'BbClarinet_ff',\n",
    "                          'Cello_arco_ff_sulA',\n",
    "                          'Cello_arco_ff_sulC',\n",
    "                          'Cello_arco_ff_sulD',\n",
    "                          'Cello_arco_ff_sulG',\n",
    "                          'Cello_pizz_ff_sulA',\n",
    "                          'Cello_pizz_ff_sulC',\n",
    "                          'Cello_pizz_ff_sulD',\n",
    "                          'Cello_pizz_ff_sulG',\n",
    "                          'Crotale_ff',\n",
    "                          'EbClarinet_ff',\n",
    "                          'Flute_nonvib_ff',\n",
    "                          'Horn_ff',\n",
    "                          'Marimba_cord_ff',\n",
    "                          'Marimba_roll_ff',\n",
    "                          'Marimba_rubber_ff',\n",
    "                          'Oboe_ff',\n",
    "                          'TenorTrombone_ff',\n",
    "                          'Trumpet_novib_ff',\n",
    "                          'Trumpet_vib_ff',\n",
    "                          'Tuba_ff',\n",
    "                          'Vibraphone_bow',\n",
    "                          'Vibraphone_dampen_ff',\n",
    "                          'Vibraphone_shortsustain_ff',\n",
    "                          'Viola_arco_ff_sulA',\n",
    "                          'Viola_arco_ff_sulC',\n",
    "                          'Viola_arco_ff_sulD',\n",
    "                          'Viola_arco_ff_sulG',\n",
    "                          'Viola_pizz_ff_sulA',\n",
    "                          'Viola_pizz_ff_sulC',\n",
    "                          'Viola_pizz_ff_sulD',\n",
    "                          'Viola_pizz_ff_sulG',\n",
    "                          'Violin_pizz_ff_sulA',\n",
    "                          'Violin_pizz_ff_sulD',\n",
    "                          'Violin_pizz_ff_sulE',\n",
    "                          'Violin_pizz_ff_sulG',\n",
    "                          'Xylophone_hardrubber_ff',\n",
    "                          'Xylophone_hardrubber_roll_ff',\n",
    "                          'Xylophone_rosewood_ff',\n",
    "                          'bells_brass_ff',\n",
    "                          'bells_plastic_ff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundfont = {}\n",
    "soundbank_ref = []\n",
    "k = 0\n",
    "for inst in restricted_instruments:\n",
    "    soundfont[inst] = {}\n",
    "    for i in range(128):\n",
    "        if instruments[inst]['min_note'] <= i <= instruments[inst]['max_note']:\n",
    "            soundfont[inst][i] = (k, instruments[inst]['samples'][i])\n",
    "            soundbank_ref.append((inst, i))\n",
    "            k += 1\n",
    "        else:\n",
    "            soundfont[inst][i] = None\n",
    "            \n",
    "soundbank = [soundfont[x[0]][x[1]][1] for x in soundbank_ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_endo_matrix(size=len(soundbank)):\n",
    "    matrix = np.diag([1 for i in range(size)])\n",
    "    idx = np.random.randint(size, size=size)\n",
    "    return matrix[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "class Song:\n",
    "    def __init__(self, sample_bank, length_in_beats, bpm, sr):\n",
    "        self.sample_bank = sample_bank\n",
    "        self.num_samples = len(sample_bank)\n",
    "        self.length_in_beats = length_in_beats\n",
    "        self.bpm = bpm\n",
    "        self.sr = sr\n",
    "        self.notes = np.zeros([length_in_beats, len(sample_bank)], dtype=bool)\n",
    "        self.wavs = []\n",
    "        for i in range(len(sample_bank)):\n",
    "            self.wavs.append(sf.read(self.sample_bank[i])[0])\n",
    "    def reset(self):\n",
    "        self.notes = np.zeros([self.length_in_beats, len(self.sample_bank)], dtype=bool)\n",
    "    def generate(self):\n",
    "        samples_per_beat = (self.sr * 60) // self.bpm\n",
    "        num_samples = samples_per_beat * self.length_in_beats\n",
    "        self.output = np.zeros(num_samples)\n",
    "        for i in range(self.length_in_beats):\n",
    "            for j in range(self.num_samples):\n",
    "                if self.notes[i][j]:\n",
    "                    N = self.sr * self.length_in_beats * 60 // self.bpm\n",
    "                    sample = self.wavs[j][:N,0]\n",
    "                    padded_sample = np.zeros(num_samples)\n",
    "                    padded_sample[:sample.shape[0]] = sample\n",
    "                    self.output += np.roll(padded_sample, i*samples_per_beat)\n",
    "                    self.output = self.output\n",
    "    def add(self, i, j):\n",
    "        self.notes[i][j] = 1\n",
    "    def transform(self, matrix):\n",
    "        for i in range(self.notes.shape[0]):\n",
    "            self.notes[i] = np.matmul(matrix, self.notes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_part(instruments, song):\n",
    "    length = 3 * song.length_in_beats // 4\n",
    "    lst = [k for k in list(range(len(song.sample_bank))) if soundbank_ref[k][0] in instruments]\n",
    "    for i in range(length):\n",
    "        make_note = np.random.choice([0,0,0,1])\n",
    "        if make_note:\n",
    "            j = np.random.choice(lst)\n",
    "            song.add(i, j)\n",
    "    #song.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "song = Song(soundbank, 60, 240, 44100)\n",
    "\n",
    "def create_example(song):\n",
    "    song.reset()\n",
    "    generate_random_part(restricted_instruments, song)\n",
    "    song.generate()\n",
    "    transformation_matrix = random_endo_matrix()\n",
    "    coded_transformation = np.argmax(transformation_matrix, axis=0)\n",
    "    features = copy.deepcopy(np.concatenate([song.output, coded_transformation]))\n",
    "    song.transform(transformation_matrix)\n",
    "    song.generate()\n",
    "    target = copy.deepcopy(song.output)\n",
    "    return features, target\n",
    "\n",
    "def create_examples(num_examples, song):\n",
    "    features_list = []\n",
    "    target_list = []\n",
    "    for i in range(num_examples):\n",
    "        features, target = create_example(song)\n",
    "        features_list.append(features)\n",
    "        target_list.append(target)\n",
    "    return np.array(features_list), np.array(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io.wavfile import write as wav_write, read as wav_read\n",
    "\n",
    "def load_example(index):\n",
    "    transformation_matrix = np.loadtxt(f'input{index}.csv', delimiter=',')\n",
    "    coded_transformation = np.argmax(transformation_matrix, axis=0)\n",
    "    _, input_signal = wav_read(f'input{index}.wav')\n",
    "    _, output_signal = wav_read(f'output{index}.wav')\n",
    "    x2 = np.reshape(transformation_matrix, [1721*1721])\n",
    "    #features = np.concatenate([input_signal, np.reshape(transformation_matrix, 1721*1721)])\n",
    "    features = np.concatenate([input_signal[:15*44100], coded_transformation])\n",
    "    target = output_signal[:15*44100]\n",
    "    return features, target\n",
    "\n",
    "def load_examples(indices):\n",
    "    features_list = []\n",
    "    target_list = []\n",
    "    for idx in indices:\n",
    "        features, target = load_example(idx)\n",
    "        features_list.append(features)\n",
    "        target_list.append(target)\n",
    "    return np.array(features_list), np.array(target_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfccs(pcm):\n",
    "\n",
    "    # sample_rate = 44100            # 44.1k samples per channel per second\n",
    "    # num_samples = 60*sample_rate   # 60 seconds of data\n",
    "    # num_channels = 2               # 2 channels\n",
    "    # # Input is Tensor of [batch_size, num_samples, num_channels] PCM samples in the range [-1, 1].\n",
    "    # pcm = tf.compat.v1.placeholder(tf.float32, [None, num_channels, num_samples])\n",
    "\n",
    "    # A 2048-point STFT with frames of ??? ms and 75% overlap.\n",
    "    stfts = tf.signal.stft(pcm, frame_length=2048, frame_step=256, fft_length=2048)\n",
    "    spectrograms = tf.abs(stfts)\n",
    "\n",
    "    # Warp the linear scale spectrograms into the mel-scale.\n",
    "    num_spectrogram_bins = stfts.shape[-1].value\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins,\n",
    "                                                                        num_spectrogram_bins,\n",
    "                                                                        sample_rate,\n",
    "                                                                        lower_edge_hertz,\n",
    "                                                                        upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    # Compute MFCCs from log_mel_spectrograms and take the first 13.\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :13]\n",
    "    return mfccs\n",
    "\n",
    "def mfccs_loss(pcm_true, pcm_pred):\n",
    "    mfccs_true = mfccs(pcm_true)\n",
    "    mfccs_pred = mfccs(pcm_pred)\n",
    "    return tf.losses.mean_squared_error(labels=mfccs_true, predictions=mfccs_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/flatrionschool/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000000  train loss 12916.7637\n",
      "Batch 00000000  test loss 15864.3291\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000001  train loss 16211.9023\n",
      "Batch 00000001  test loss 14737.2344\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000002  train loss 12810.1172\n",
      "Batch 00000002  test loss 12363.9219\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000003  train loss 11556.5449\n",
      "Batch 00000003  test loss 12308.2148\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000004  train loss 10760.9277\n",
      "Batch 00000004  test loss 10486.1172\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000005  train loss 15837.4902\n",
      "Batch 00000005  test loss 14489.5713\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000006  train loss 11849.4814\n",
      "Batch 00000006  test loss 11808.4170\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000007  train loss 2032.7306\n",
      "Batch 00000007  test loss 2173.8909\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000008  train loss 2280.5237\n",
      "Batch 00000008  test loss 3639.2275\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000009  train loss 1724.2217\n",
      "Batch 00000009  test loss 1349.4910\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000010  train loss 2453.9690\n",
      "Batch 00000010  test loss 1831.8483\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000011  train loss 901.2678\n",
      "Batch 00000011  test loss 510.6539\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000012  train loss 767.9756\n",
      "Batch 00000012  test loss 428.2892\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000013  train loss 716.0030\n",
      "Batch 00000013  test loss 826.8395\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000014  train loss 671.5381\n",
      "Batch 00000014  test loss 596.4331\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000015  train loss 348.9045\n",
      "Batch 00000015  test loss 1134.5211\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000016  train loss 257.2397\n",
      "Batch 00000016  test loss 1144.3881\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000017  train loss 480.2960\n",
      "Batch 00000017  test loss 290.5907\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000018  train loss 367.7030\n",
      "Batch 00000018  test loss 682.6319\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000019  train loss 382.5108\n",
      "Batch 00000019  test loss 914.5331\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000020  train loss 1230.8103\n",
      "Batch 00000020  test loss 1589.8705\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000021  train loss 1133.9723\n",
      "Batch 00000021  test loss 381.3152\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000022  train loss 408.1718\n",
      "Batch 00000022  test loss 1257.9929\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000023  train loss 874.8380\n",
      "Batch 00000023  test loss 433.9897\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000024  train loss 955.6669\n",
      "Batch 00000024  test loss 497.4195\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000025  train loss 1021.2090\n",
      "Batch 00000025  test loss 257.5750\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000026  train loss 652.8506\n",
      "Batch 00000026  test loss 1423.3203\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000027  train loss 350.1153\n",
      "Batch 00000027  test loss 1062.9510\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000028  train loss 873.6556\n",
      "Batch 00000028  test loss 1360.0621\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000029  train loss 606.6669\n",
      "Batch 00000029  test loss 532.3993\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000030  train loss 1534.3760\n",
      "Batch 00000030  test loss 611.0650\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000031  train loss 572.6656\n",
      "Batch 00000031  test loss 440.7233\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000032  train loss 407.0175\n",
      "Batch 00000032  test loss 417.3301\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000033  train loss 954.9749\n",
      "Batch 00000033  test loss 1236.0352\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000034  train loss 1297.8051\n",
      "Batch 00000034  test loss 528.5197\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000035  train loss 487.0440\n",
      "Batch 00000035  test loss 454.5825\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000036  train loss 893.6750\n",
      "Batch 00000036  test loss 472.9959\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000037  train loss 318.8444\n",
      "Batch 00000037  test loss 306.4735\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000038  train loss 323.7246\n",
      "Batch 00000038  test loss 369.7156\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000039  train loss 1233.5925\n",
      "Batch 00000039  test loss 472.9013\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000040  train loss 967.6603\n",
      "Batch 00000040  test loss 757.5833\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000041  train loss 620.5914\n",
      "Batch 00000041  test loss 727.9321\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000042  train loss 845.0964\n",
      "Batch 00000042  test loss 510.5514\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000043  train loss 1096.9139\n",
      "Batch 00000043  test loss 553.6451\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000044  train loss 285.3515\n",
      "Batch 00000044  test loss 816.4522\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000045  train loss 910.2756\n",
      "Batch 00000045  test loss 983.5161\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000046  train loss 879.5807\n",
      "Batch 00000046  test loss 1431.2432\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000047  train loss 615.1611\n",
      "Batch 00000047  test loss 769.6929\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000048  train loss 1478.8558\n",
      "Batch 00000048  test loss 355.0170\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000049  train loss 1338.2152\n",
      "Batch 00000049  test loss 297.3176\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000050  train loss 293.9204\n",
      "Batch 00000050  test loss 1050.8832\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000051  train loss 464.2771\n",
      "Batch 00000051  test loss 281.5876\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000052  train loss 941.8224\n",
      "Batch 00000052  test loss 445.8432\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000053  train loss 1235.6193\n",
      "Batch 00000053  test loss 1683.4023\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000054  train loss 819.2972\n",
      "Batch 00000054  test loss 997.0009\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000055  train loss 619.2069\n",
      "Batch 00000055  test loss 849.0220\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000056  train loss 637.3484\n",
      "Batch 00000056  test loss 855.7534\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000057  train loss 397.7788\n",
      "Batch 00000057  test loss 367.7433\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000058  train loss 1162.4633\n",
      "Batch 00000058  test loss 693.1660\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000059  train loss 299.7440\n",
      "Batch 00000059  test loss 852.2974\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000060  train loss 1549.1259\n",
      "Batch 00000060  test loss 954.5955\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000061  train loss 1170.5176\n",
      "Batch 00000061  test loss 1304.9452\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000062  train loss 1814.2683\n",
      "Batch 00000062  test loss 401.2755\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000063  train loss 1325.0841\n",
      "Batch 00000063  test loss 1002.2212\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000064  train loss 805.2881\n",
      "Batch 00000064  test loss 1497.1749\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000065  train loss 656.3848\n",
      "Batch 00000065  test loss 442.4018\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000066  train loss 832.0448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00000066  test loss 1490.4473\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000067  train loss 1465.5050\n",
      "Batch 00000067  test loss 1021.3139\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000068  train loss 411.9760\n",
      "Batch 00000068  test loss 897.3539\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000069  train loss 621.2734\n",
      "Batch 00000069  test loss 615.9865\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000070  train loss 552.1841\n",
      "Batch 00000070  test loss 155.0680\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000071  train loss 191.7017\n",
      "Batch 00000071  test loss 450.6239\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000072  train loss 469.6700\n",
      "Batch 00000072  test loss 666.1762\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000073  train loss 1401.8611\n",
      "Batch 00000073  test loss 834.5666\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000074  train loss 1509.1620\n",
      "Batch 00000074  test loss 242.1435\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000075  train loss 504.8276\n",
      "Batch 00000075  test loss 942.2906\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000076  train loss 855.0412\n",
      "Batch 00000076  test loss 1000.0546\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000077  train loss 408.0662\n",
      "Batch 00000077  test loss 1333.1687\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000078  train loss 1336.7402\n",
      "Batch 00000078  test loss 1610.5880\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000079  train loss 779.7014\n",
      "Batch 00000079  test loss 226.5783\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000080  train loss 434.7566\n",
      "Batch 00000080  test loss 253.2257\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000081  train loss 747.5718\n",
      "Batch 00000081  test loss 407.7300\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000082  train loss 1732.4213\n",
      "Batch 00000082  test loss 293.6989\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000083  train loss 482.4668\n",
      "Batch 00000083  test loss 1267.5527\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000084  train loss 237.6477\n",
      "Batch 00000084  test loss 415.2864\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000085  train loss 785.1762\n",
      "Batch 00000085  test loss 1463.6924\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000086  train loss 584.7896\n",
      "Batch 00000086  test loss 1188.7974\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000087  train loss 254.2494\n",
      "Batch 00000087  test loss 1644.0499\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000088  train loss 1407.4692\n",
      "Batch 00000088  test loss 667.1478\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000089  train loss 386.5215\n",
      "Batch 00000089  test loss 630.5128\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000090  train loss 771.3231\n",
      "Batch 00000090  test loss 572.4579\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000091  train loss 404.8914\n",
      "Batch 00000091  test loss 899.6945\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000092  train loss 488.1666\n",
      "Batch 00000092  test loss 1167.3850\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000093  train loss 1602.2617\n",
      "Batch 00000093  test loss 1370.6283\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000094  train loss 471.3612\n",
      "Batch 00000094  test loss 479.1485\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000095  train loss 862.0764\n",
      "Batch 00000095  test loss 399.9279\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000096  train loss 523.7574\n",
      "Batch 00000096  test loss 1585.4788\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000097  train loss 313.0588\n",
      "Batch 00000097  test loss 581.2646\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000098  train loss 2012.8522\n",
      "Batch 00000098  test loss 290.8357\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000099  train loss 458.1435\n",
      "Batch 00000099  test loss 669.6376\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000100  train loss 788.0130\n",
      "Batch 00000100  test loss 1104.5433\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000101  train loss 657.8312\n",
      "Batch 00000101  test loss 1110.0188\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000102  train loss 369.3407\n",
      "Batch 00000102  test loss 1486.8304\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000103  train loss 840.8953\n",
      "Batch 00000103  test loss 695.8734\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000104  train loss 755.7131\n",
      "Batch 00000104  test loss 521.5541\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000105  train loss 1254.3062\n",
      "Batch 00000105  test loss 1458.1239\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000106  train loss 307.4175\n",
      "Batch 00000106  test loss 312.1872\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000107  train loss 1574.2775\n",
      "Batch 00000107  test loss 226.7515\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000108  train loss 696.2744\n",
      "Batch 00000108  test loss 404.3960\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000109  train loss 451.6867\n",
      "Batch 00000109  test loss 1317.2394\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000110  train loss 560.9586\n",
      "Batch 00000110  test loss 349.5869\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000111  train loss 947.7826\n",
      "Batch 00000111  test loss 980.9060\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000112  train loss 834.5880\n",
      "Batch 00000112  test loss 894.2908\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000113  train loss 390.8452\n",
      "Batch 00000113  test loss 370.9971\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000114  train loss 360.0847\n",
      "Batch 00000114  test loss 420.3560\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000115  train loss 1055.9995\n",
      "Batch 00000115  test loss 414.1426\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000116  train loss 565.6707\n",
      "Batch 00000116  test loss 910.2802\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000117  train loss 726.5712\n",
      "Batch 00000117  test loss 1294.4954\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000118  train loss 859.8637\n",
      "Batch 00000118  test loss 457.4295\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000119  train loss 312.3818\n",
      "Batch 00000119  test loss 647.0430\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000120  train loss 851.7888\n",
      "Batch 00000120  test loss 1349.6727\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000121  train loss 1669.3153\n",
      "Batch 00000121  test loss 401.4695\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000122  train loss 481.2090\n",
      "Batch 00000122  test loss 1013.6365\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000123  train loss 2521.3242\n",
      "Batch 00000123  test loss 462.2624\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000124  train loss 241.9401\n",
      "Batch 00000124  test loss 557.1064\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000125  train loss 1485.0437\n",
      "Batch 00000125  test loss 1417.2570\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000126  train loss 1962.7097\n",
      "Batch 00000126  test loss 366.7453\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000127  train loss 674.6974\n",
      "Batch 00000127  test loss 675.4789\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000128  train loss 1069.9269\n",
      "Batch 00000128  test loss 619.3536\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000129  train loss 323.9106\n",
      "Batch 00000129  test loss 1367.5902\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000130  train loss 1291.5612\n",
      "Batch 00000130  test loss 371.6810\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000131  train loss 385.8814\n",
      "Batch 00000131  test loss 953.0029\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000132  train loss 585.2341\n",
      "Batch 00000132  test loss 853.5369\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000133  train loss 436.0389\n",
      "Batch 00000133  test loss 270.8561\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000134  train loss 743.4606\n",
      "Batch 00000134  test loss 695.0851\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000135  train loss 641.2477\n",
      "Batch 00000135  test loss 1030.6786\n",
      "Generated New Examples\n",
      "New Exampled Generated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 00000136  train loss 841.1611\n",
      "Batch 00000136  test loss 438.0805\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000137  train loss 329.6581\n",
      "Batch 00000137  test loss 1025.5394\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000138  train loss 646.5123\n",
      "Batch 00000138  test loss 282.8802\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000139  train loss 383.2646\n",
      "Batch 00000139  test loss 792.8745\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000140  train loss 715.5602\n",
      "Batch 00000140  test loss 610.4487\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000141  train loss 835.7424\n",
      "Batch 00000141  test loss 492.4175\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000142  train loss 822.0158\n",
      "Batch 00000142  test loss 385.4799\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000143  train loss 2113.7498\n",
      "Batch 00000143  test loss 2231.4475\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000144  train loss 1214.0676\n",
      "Batch 00000144  test loss 725.1464\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000145  train loss 722.9642\n",
      "Batch 00000145  test loss 621.4684\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000146  train loss 1722.0679\n",
      "Batch 00000146  test loss 329.8076\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000147  train loss 256.2017\n",
      "Batch 00000147  test loss 1002.1998\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000148  train loss 1074.4818\n",
      "Batch 00000148  test loss 591.7681\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000149  train loss 651.0469\n",
      "Batch 00000149  test loss 594.2447\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000150  train loss 1031.7638\n",
      "Batch 00000150  test loss 510.0677\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000151  train loss 480.7567\n",
      "Batch 00000151  test loss 259.0451\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000152  train loss 1496.1929\n",
      "Batch 00000152  test loss 608.6017\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000153  train loss 310.8904\n",
      "Batch 00000153  test loss 542.6024\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000154  train loss 441.9205\n",
      "Batch 00000154  test loss 332.0374\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000155  train loss 923.5068\n",
      "Batch 00000155  test loss 191.3458\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000156  train loss 832.3202\n",
      "Batch 00000156  test loss 874.8431\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000157  train loss 382.9715\n",
      "Batch 00000157  test loss 1106.5497\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000158  train loss 825.0507\n",
      "Batch 00000158  test loss 993.3292\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000159  train loss 185.0489\n",
      "Batch 00000159  test loss 1649.0002\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000160  train loss 1360.7002\n",
      "Batch 00000160  test loss 775.1930\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000161  train loss 563.1154\n",
      "Batch 00000161  test loss 460.5723\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000162  train loss 391.7619\n",
      "Batch 00000162  test loss 1086.1071\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000163  train loss 1538.4691\n",
      "Batch 00000163  test loss 1072.1064\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000164  train loss 569.0369\n",
      "Batch 00000164  test loss 607.3815\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000165  train loss 1014.7069\n",
      "Batch 00000165  test loss 410.4612\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000166  train loss 784.1555\n",
      "Batch 00000166  test loss 326.9072\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000167  train loss 713.6120\n",
      "Batch 00000167  test loss 956.8510\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000168  train loss 1182.2223\n",
      "Batch 00000168  test loss 901.6777\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000169  train loss 354.0137\n",
      "Batch 00000169  test loss 760.3593\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000170  train loss 1166.1396\n",
      "Batch 00000170  test loss 1184.1268\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000171  train loss 577.9413\n",
      "Batch 00000171  test loss 343.8925\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000172  train loss 1388.1210\n",
      "Batch 00000172  test loss 906.0516\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000173  train loss 1120.5800\n",
      "Batch 00000173  test loss 1243.5629\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000174  train loss 554.7255\n",
      "Batch 00000174  test loss 484.3132\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000175  train loss 836.7386\n",
      "Batch 00000175  test loss 537.4310\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000176  train loss 338.2288\n",
      "Batch 00000176  test loss 1179.1721\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000177  train loss 1482.5247\n",
      "Batch 00000177  test loss 1467.9492\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000178  train loss 817.8265\n",
      "Batch 00000178  test loss 1111.2159\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000179  train loss 701.5639\n",
      "Batch 00000179  test loss 1063.9011\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000180  train loss 488.4936\n",
      "Batch 00000180  test loss 346.6594\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000181  train loss 1287.3296\n",
      "Batch 00000181  test loss 177.8649\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000182  train loss 325.2613\n",
      "Batch 00000182  test loss 347.6809\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000183  train loss 318.5535\n",
      "Batch 00000183  test loss 286.7509\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000184  train loss 775.8680\n",
      "Batch 00000184  test loss 413.9451\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000185  train loss 514.5836\n",
      "Batch 00000185  test loss 299.1475\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000186  train loss 712.4253\n",
      "Batch 00000186  test loss 376.8711\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000187  train loss 787.3409\n",
      "Batch 00000187  test loss 1202.5853\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000188  train loss 1054.1315\n",
      "Batch 00000188  test loss 520.2372\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000189  train loss 382.8050\n",
      "Batch 00000189  test loss 223.8278\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000190  train loss 1331.4578\n",
      "Batch 00000190  test loss 357.6330\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000191  train loss 895.7250\n",
      "Batch 00000191  test loss 343.0572\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000192  train loss 709.0058\n",
      "Batch 00000192  test loss 991.8469\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000193  train loss 1260.3955\n",
      "Batch 00000193  test loss 734.9354\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000194  train loss 1422.3042\n",
      "Batch 00000194  test loss 473.9466\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000195  train loss 1182.3136\n",
      "Batch 00000195  test loss 2271.0029\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000196  train loss 729.7076\n",
      "Batch 00000196  test loss 317.3055\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000197  train loss 202.2770\n",
      "Batch 00000197  test loss 1226.2289\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000198  train loss 948.1395\n",
      "Batch 00000198  test loss 1600.8833\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000199  train loss 311.8105\n",
      "Batch 00000199  test loss 727.2255\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000200  train loss 494.2929\n",
      "Batch 00000200  test loss 909.9265\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000201  train loss 687.2612\n",
      "Batch 00000201  test loss 423.8943\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000202  train loss 587.9501\n",
      "Batch 00000202  test loss 442.2286\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000203  train loss 554.5894\n",
      "Batch 00000203  test loss 1876.0167\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000204  train loss 258.9255\n",
      "Batch 00000204  test loss 485.7086\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000205  train loss 1260.2816\n",
      "Batch 00000205  test loss 949.7227\n",
      "Generated New Examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Exampled Generated\n",
      "Batch 00000206  train loss 327.8693\n",
      "Batch 00000206  test loss 863.5515\n",
      "Generated New Examples\n",
      "New Exampled Generated\n",
      "Batch 00000207  train loss 704.0002\n",
      "Batch 00000207  test loss 1460.6371\n",
      "Generated New Examples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ce971f57247f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m#X, Y = load_examples(np.random.randint(200, size=10))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Generated New Examples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'New Exampled Generated'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mfrac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ee032f248052>\u001b[0m in \u001b[0;36mcreate_examples\u001b[0;34m(num_examples, song)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtarget_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mfeatures_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtarget_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ee032f248052>\u001b[0m in \u001b[0;36mcreate_example\u001b[0;34m(song)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcoded_transformation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformation_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msong\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoded_transformation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msong\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformation_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0msong\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msong\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-47039426e14c>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, matrix)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "\n",
    "# create the TF neural net\n",
    "# some hyperparams\n",
    "training_batches = 2000\n",
    "\n",
    "n_neurons_in_h1 = 200\n",
    "n_neurons_in_h2 = 200\n",
    "n_neurons_in_h3 = 200\n",
    "n_neurons_in_h4 = 200\n",
    "n_neurons_in_h5 = 200\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "sample_rate = 44100            # 44.1k samples per channel per second\n",
    "num_samples = 15*sample_rate   # 15 seconds of data\n",
    "\n",
    "num_sounds = 1721\n",
    "\n",
    "#n_features = num_samples + (num_sounds)**2\n",
    "n_features = num_samples + num_sounds\n",
    "n_targets = num_samples\n",
    "#############################################\n",
    "\n",
    "# basic 2 layer dense net (MLP) example adapted from\n",
    "# https://becominghuman.ai/creating-your-own-neural-network-using-tensorflow-fa8ca7cc4d0e\n",
    "\n",
    "# these placeholders serve as our input tensors\n",
    "x = tf.placeholder(tf.float32, [None, n_features], name='input')\n",
    "#t = tf.placeholder(tf.float32, [None, (69+127)*(69+127)], name='tone_transformation_tensor')\n",
    "y = tf.placeholder(tf.float32, [None, n_targets], name='labels')\n",
    "\n",
    "# TF Variables are our neural net parameter tensors, we initialize them to random (gaussian) values in\n",
    "# Layer1. Variables are allowed to be persistent across training epochs and updatable bt TF operations\n",
    "W1 = tf.Variable(tf.truncated_normal([n_features, n_neurons_in_h1], mean=0, stddev=1 / np.sqrt(n_features)),\n",
    "                 name='weights1')\n",
    "b1 = tf.Variable(tf.truncated_normal([n_neurons_in_h1], mean=0, stddev=1 / np.sqrt(n_features)), name='biases1')\n",
    "\n",
    "# note the output tensor of the 1st layer is the activation applied to a\n",
    "# linear transform of the layer 1 parameter tensors\n",
    "# the matmul operation calculates the dot product between the tensors\n",
    "y1 = tf.nn.relu((tf.matmul(x, W1) + b1), name='activationLayer1')\n",
    "\n",
    "# network parameters(weights and biases) are set and initialized (Layer2)\n",
    "W2 = tf.Variable(tf.random_normal([n_neurons_in_h1, n_neurons_in_h2], mean=0, stddev=1),\n",
    "                 name='weights2')\n",
    "b2 = tf.Variable(tf.random_normal([n_neurons_in_h2], mean=0, stddev=1), name='biases2')\n",
    "# activation function(sigmoid)\n",
    "y2 = tf.nn.relu((tf.matmul(y1, W2) + b2), name='activationLayer2')\n",
    "\n",
    "# network parameters(weights and biases) are set and initialized (Layer3)\n",
    "W3 = tf.Variable(tf.random_normal([n_neurons_in_h2, n_neurons_in_h3], mean=0, stddev=1),\n",
    "                 name='weights2')\n",
    "b3 = tf.Variable(tf.random_normal([n_neurons_in_h3], mean=0, stddev=1), name='biases3')\n",
    "# activation function(sigmoid)\n",
    "y3 = tf.nn.relu((tf.matmul(y2, W3) + b3), name='activationLayer2')\n",
    "\n",
    "# network parameters(weights and biases) are set and initialized (Layer4)\n",
    "W4 = tf.Variable(tf.random_normal([n_neurons_in_h3, n_neurons_in_h4], mean=0, stddev=1),\n",
    "                 name='weights2')\n",
    "b4 = tf.Variable(tf.random_normal([n_neurons_in_h4], mean=0, stddev=1), name='biases4')\n",
    "# activation function(sigmoid)\n",
    "y4 = tf.nn.relu((tf.matmul(y3, W4) + b4), name='activationLayer2')\n",
    "\n",
    "# network parameters(weights and biases) are set and initialized (Layer5)\n",
    "W5 = tf.Variable(tf.random_normal([n_neurons_in_h4, n_neurons_in_h5], mean=0, stddev=1),\n",
    "                 name='weights2')\n",
    "b5 = tf.Variable(tf.random_normal([n_neurons_in_h5], mean=0, stddev=1), name='biases5')\n",
    "# activation function(sigmoid)\n",
    "y5 = tf.nn.relu((tf.matmul(y4, W5) + b5), name='activationLayer2')\n",
    "\n",
    "# output layer weights and biases\n",
    "Wo = tf.Variable(tf.random_normal([n_neurons_in_h5, n_targets], mean=0, stddev=1 ),\n",
    "                 name='weightsOut')\n",
    "bo = tf.Variable(tf.random_normal([n_targets], mean=0, stddev=1), name='biasesOut')\n",
    "\n",
    "# the sigmoid (binary softmax) activation is absorbed into TF's sigmoid_cross_entropy_with_logits loss\n",
    "#logits = (tf.matmul(y2, Wo) + bo)\n",
    "output = tf.nn.relu(tf.matmul(y5, Wo) + bo)\n",
    "#loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "loss = mfccs_loss(y, output)\n",
    "\n",
    "# tap a separate output that applies softmax activation to the output layer\n",
    "# for training accuracy readout\n",
    "#a = tf.nn.sigmoid(logits, name='activationOutputLayer')\n",
    "\n",
    "# optimizer used to compute gradient of loss and apply the parameter updates.\n",
    "# the train_step object returned is ran by a TF Session to train the net\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# prediction accuracy\n",
    "# compare predicted value from network with the expected value/target\n",
    "\n",
    "#correct_prediction = tf.equal(tf.round(a), y)\n",
    "# accuracy determination\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\")\n",
    "\n",
    "#############################################\n",
    "# ***NOTE global_variables_initializer() must be called before creating a tf.Session()!***\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# create a session for training and feedforward (prediction). Sessions are TF's way to run\n",
    "# feed data to placeholders and variables, obtain outputs and update neural net parameters\n",
    "with tf.Session() as sess:\n",
    "    # ***initialization of all variables... NOTE this must be done before running any further sessions!***\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # training loop over the number of epochs\n",
    "    batches = 2000\n",
    "\n",
    "    for batch in range(training_batches):\n",
    "        losses = 0\n",
    "        accs = 0\n",
    "        \n",
    "        #X, Y = load_examples(np.random.randint(200, size=10))\n",
    "        print('Generating New Examples')\n",
    "        X, Y = create_examples(2, song)\n",
    "        print('New Exampled Generated')\n",
    "        frac = 0.5\n",
    "        train_stop = int(len(X) * frac)\n",
    "        X_train = X[:train_stop]\n",
    "        Y_train = Y[:train_stop]\n",
    "        X_test = X[train_stop:]\n",
    "        Y_test = Y[train_stop:]\n",
    "        \n",
    "        X_b = X_train\n",
    "        Y_b = Y_train\n",
    "\n",
    "        # train the network, note the dictionary of inputs and labels\n",
    "        sess.run(train_step, feed_dict={x: X_b, y: Y_b})\n",
    "        # feedforwad the same data and labels, but grab the accuracy and loss as outputs\n",
    "        l = sess.run([loss], feed_dict={x: X_b, y: Y_b})\n",
    "\n",
    "        losses = np.sum(l)\n",
    "        print(\"Batch %.8d \" % batch, \"train loss %.4f\" % losses)\n",
    "\n",
    "        # test on the holdout set\n",
    "        test_output, l = sess.run([output, loss], feed_dict={x: X_test, y: Y_test})\n",
    "        losses = np.sum(l)\n",
    "        print(\"Batch %.8d \" % batch, \"test loss %.4f\" % losses)\n",
    "        if (batch % 5 == 0):\n",
    "            sd.play(test_output[0], 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
