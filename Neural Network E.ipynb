{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, getcwd, rename, remove\n",
    "from os.path import isfile, join\n",
    "\n",
    "path = join(getcwd(), 'preprocessed_samples')\n",
    "\n",
    "files = [f for f in listdir(path) if isfile(join(path, f)) and f[0] != '.']\n",
    "\n",
    "samples = []\n",
    "for f in files:\n",
    "    sample = {'instrument' : '_'.join(f.split('_')[:-1]),\n",
    "              'midi_number' : int(f.split('_')[-1][:-4]),\n",
    "              'filename' : join(path, f)}\n",
    "    samples.append(sample)\n",
    "\n",
    "samples = sorted(samples, key=lambda x: (x['instrument'], x['midi_number']))\n",
    "    \n",
    "instrument_names = set([s['instrument'] for s in samples])\n",
    "print(instrument_names)\n",
    "instruments = {}\n",
    "for inst in instrument_names:\n",
    "    samples_for_inst = {s['midi_number'] : s['filename'] for s in samples if s['instrument'] == inst}\n",
    "    instruments[inst] = {\n",
    "        'samples' : samples_for_inst,\n",
    "        'min_note' : min(samples_for_inst.keys()),\n",
    "        'max_note' : max(samples_for_inst.keys())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_instruments = ['AltoSax_NoVib_ff',\n",
    "                          'BassClarinet_ff',\n",
    "                          'BassFlute_ff',\n",
    "                          'BassTrombone_ff',\n",
    "                          'Bass_arco_ff_sulA',\n",
    "                          'Bass_arco_ff_sulD',\n",
    "                          'Bass_arco_ff_sulE',\n",
    "                          'Bass_arco_ff_sulG',\n",
    "                          'Bass_pizz_ff_sulA',\n",
    "                          'Bass_pizz_ff_sulD',\n",
    "                          'Bass_pizz_ff_sulE',\n",
    "                          'Bass_pizz_ff_sulG',\n",
    "                          'BbClarinet_ff',\n",
    "                          'Cello_arco_ff_sulA',\n",
    "                          'Cello_arco_ff_sulC',\n",
    "                          'Cello_arco_ff_sulD',\n",
    "                          'Cello_arco_ff_sulG',\n",
    "                          'Cello_pizz_ff_sulA',\n",
    "                          'Cello_pizz_ff_sulC',\n",
    "                          'Cello_pizz_ff_sulD',\n",
    "                          'Cello_pizz_ff_sulG',\n",
    "                          'Crotale_ff',\n",
    "                          'EbClarinet_ff',\n",
    "                          'Flute_nonvib_ff',\n",
    "                          'Horn_ff',\n",
    "                          'Marimba_cord_ff',\n",
    "                          'Marimba_roll_ff',\n",
    "                          'Marimba_rubber_ff',\n",
    "                          'Oboe_ff',\n",
    "                          'TenorTrombone_ff',\n",
    "                          'Trumpet_novib_ff',\n",
    "                          'Trumpet_vib_ff',\n",
    "                          'Tuba_ff',\n",
    "                          'Vibraphone_bow',\n",
    "                          'Vibraphone_dampen_ff',\n",
    "                          'Vibraphone_shortsustain_ff',\n",
    "                          'Viola_arco_ff_sulA',\n",
    "                          'Viola_arco_ff_sulC',\n",
    "                          'Viola_arco_ff_sulD',\n",
    "                          'Viola_arco_ff_sulG',\n",
    "                          'Viola_pizz_ff_sulA',\n",
    "                          'Viola_pizz_ff_sulC',\n",
    "                          'Viola_pizz_ff_sulD',\n",
    "                          'Viola_pizz_ff_sulG',\n",
    "                          'Violin_pizz_ff_sulA',\n",
    "                          'Violin_pizz_ff_sulD',\n",
    "                          'Violin_pizz_ff_sulE',\n",
    "                          'Violin_pizz_ff_sulG',\n",
    "                          'Xylophone_hardrubber_ff',\n",
    "                          'Xylophone_hardrubber_roll_ff',\n",
    "                          'Xylophone_rosewood_ff',\n",
    "                          'bells_brass_ff',\n",
    "                          'bells_plastic_ff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundfont = {}\n",
    "soundbank_ref = []\n",
    "k = 0\n",
    "for inst in restricted_instruments:\n",
    "    soundfont[inst] = {}\n",
    "    for i in range(128):\n",
    "        if instruments[inst]['min_note'] <= i <= instruments[inst]['max_note']:\n",
    "            soundfont[inst][i] = (k, instruments[inst]['samples'][i])\n",
    "            soundbank_ref.append((inst, i))\n",
    "            k += 1\n",
    "        else:\n",
    "            soundfont[inst][i] = None\n",
    "            \n",
    "soundbank = [soundfont[x[0]][x[1]][1] for x in soundbank_ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_endo_matrix(size=len(soundbank)):\n",
    "    matrix = np.diag([1 for i in range(size)])\n",
    "    idx = np.random.randint(size, size=size)\n",
    "    return matrix[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "class Song:\n",
    "    def __init__(self, sample_bank, length_in_beats, bpm, sr):\n",
    "        self.sample_bank = sample_bank\n",
    "        self.num_samples = len(sample_bank)\n",
    "        self.length_in_beats = length_in_beats\n",
    "        self.bpm = bpm\n",
    "        self.sr = sr\n",
    "        self.notes = np.zeros([length_in_beats, len(sample_bank)], dtype=bool)\n",
    "        self.wavs = []\n",
    "        for i in range(len(sample_bank)):\n",
    "            self.wavs.append(sf.read(self.sample_bank[i])[0])\n",
    "    def reset(self):\n",
    "        self.notes = np.zeros([self.length_in_beats, len(self.sample_bank)], dtype=bool)\n",
    "    def generate(self):\n",
    "        samples_per_beat = (self.sr * 60) // self.bpm\n",
    "        num_samples = samples_per_beat * self.length_in_beats\n",
    "        self.output = np.zeros(num_samples)\n",
    "        for i in range(self.length_in_beats):\n",
    "            for j in range(self.num_samples):\n",
    "                if self.notes[i][j]:\n",
    "                    N = self.sr * self.length_in_beats * 60 // self.bpm\n",
    "                    sample = self.wavs[j][:N,0]\n",
    "                    padded_sample = np.zeros(num_samples)\n",
    "                    padded_sample[:sample.shape[0]] = sample\n",
    "                    self.output += np.roll(padded_sample, i*samples_per_beat)\n",
    "                    self.output = self.output\n",
    "    def add(self, i, j):\n",
    "        self.notes[i][j] = 1\n",
    "    def transform(self, matrix):\n",
    "        for i in range(self.notes.shape[0]):\n",
    "            self.notes[i] = np.matmul(matrix, self.notes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_part(instruments, song):\n",
    "    length = 3 * song.length_in_beats // 4\n",
    "    lst = [k for k in list(range(len(song.sample_bank))) if soundbank_ref[k][0] in instruments]\n",
    "    for i in range(length):\n",
    "        make_note = np.random.choice([0,0,0,1])\n",
    "        if make_note:\n",
    "            j = np.random.choice(lst)\n",
    "            song.add(i, j)\n",
    "    #song.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "song = Song(soundbank, 60, 240, 44100)\n",
    "\n",
    "def create_example(song):\n",
    "    song.reset()\n",
    "    generate_random_part(restricted_instruments, song)\n",
    "    song.generate()\n",
    "    transformation_matrix = random_endo_matrix()\n",
    "    coded_transformation = np.argmax(transformation_matrix, axis=0)\n",
    "    features = copy.deepcopy(song.output)\n",
    "    song.transform(transformation_matrix)\n",
    "    song.generate()\n",
    "    target = copy.deepcopy(song.output)\n",
    "    return features, coded_transformation, target\n",
    "\n",
    "def create_examples(num_examples, song):\n",
    "    features_list = []\n",
    "    coded_transformation_list = []\n",
    "    target_list = []\n",
    "    for i in range(num_examples):\n",
    "        features, coded_transformation, target = create_example(song)\n",
    "        features_list.append(features)\n",
    "        coded_transformation_list.append(coded_transformation)\n",
    "        target_list.append(target)\n",
    "    return np.array(features_list), np.array(coded_transformation_list), np.array(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io.wavfile import write as wav_write, read as wav_read\n",
    "\n",
    "def load_example(index):\n",
    "    transformation_matrix = np.loadtxt(f'input{index}.csv', delimiter=',')\n",
    "    coded_transformation = np.argmax(transformation_matrix, axis=0)\n",
    "    _, input_signal = wav_read(f'input{index}.wav')\n",
    "    _, output_signal = wav_read(f'output{index}.wav')\n",
    "    x2 = np.reshape(transformation_matrix, [1721*1721])\n",
    "    #features = np.concatenate([input_signal, np.reshape(transformation_matrix, 1721*1721)])\n",
    "    features = np.concatenate([input_signal[:15*44100], coded_transformation])\n",
    "    target = output_signal[:15*44100]\n",
    "    return features, target\n",
    "\n",
    "def load_examples(indices):\n",
    "    features_list = []\n",
    "    target_list = []\n",
    "    for idx in indices:\n",
    "        features, target = load_example(idx)\n",
    "        features_list.append(features)\n",
    "        target_list.append(target)\n",
    "    return np.array(features_list), np.array(target_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfccs(pcm):\n",
    "\n",
    "    # sample_rate = 44100            # 44.1k samples per channel per second\n",
    "    # num_samples = 60*sample_rate   # 60 seconds of data\n",
    "    # num_channels = 2               # 2 channels\n",
    "    # # Input is Tensor of [batch_size, num_samples, num_channels] PCM samples in the range [-1, 1].\n",
    "    # pcm = tf.compat.v1.placeholder(tf.float32, [None, num_channels, num_samples])\n",
    "\n",
    "    # A 2048-point STFT with frames of ??? ms and 75% overlap.\n",
    "    stfts = tf.signal.stft(pcm, frame_length=2048, frame_step=256, fft_length=2048)\n",
    "    spectrograms = tf.abs(stfts)\n",
    "\n",
    "    # Warp the linear scale spectrograms into the mel-scale.\n",
    "    num_spectrogram_bins = stfts.shape[-1].value\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins,\n",
    "                                                                        num_spectrogram_bins,\n",
    "                                                                        sample_rate,\n",
    "                                                                        lower_edge_hertz,\n",
    "                                                                        upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    # Compute MFCCs from log_mel_spectrograms and take the first 13.\n",
    "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :13]\n",
    "    return mfccs\n",
    "\n",
    "def mfccs_loss(pcm_true, pcm_pred):\n",
    "    mfccs_true = mfccs(pcm_true)\n",
    "    mfccs_pred = mfccs(pcm_pred)\n",
    "    return tf.losses.mean_squared_error(labels=mfccs_true, predictions=mfccs_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/flatrionschool/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000000  train loss 14691.2910\n",
      "Batch 00000000  test loss 16199.7363\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000001  train loss 14658.5576\n",
      "Batch 00000001  test loss 17791.4453\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000002  train loss 16464.8281\n",
      "Batch 00000002  test loss 18322.8691\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000003  train loss 13874.2109\n",
      "Batch 00000003  test loss 14384.7227\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000004  train loss 17164.4160\n",
      "Batch 00000004  test loss 19208.6348\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000005  train loss 13526.3770\n",
      "Batch 00000005  test loss 16226.4326\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000006  train loss 13853.0908\n",
      "Batch 00000006  test loss 15265.9668\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000007  train loss 16275.6270\n",
      "Batch 00000007  test loss 18747.8613\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000008  train loss 15665.5811\n",
      "Batch 00000008  test loss 14971.2246\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000009  train loss 12887.4229\n",
      "Batch 00000009  test loss 14189.9414\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000010  train loss 18733.3281\n",
      "Batch 00000010  test loss 18054.3066\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000011  train loss 408.1174\n",
      "Batch 00000011  test loss 321.8744\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000012  train loss 927.9688\n",
      "Batch 00000012  test loss 1048.7568\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000013  train loss 1481.3306\n",
      "Batch 00000013  test loss 623.0569\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000014  train loss 725.9671\n",
      "Batch 00000014  test loss 2472.3264\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000015  train loss 538.5706\n",
      "Batch 00000015  test loss 767.3988\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000016  train loss 1148.6405\n",
      "Batch 00000016  test loss 218.6641\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000017  train loss 402.1071\n",
      "Batch 00000017  test loss 296.5141\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000018  train loss 1228.9354\n",
      "Batch 00000018  test loss 682.5198\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000019  train loss 1026.1462\n",
      "Batch 00000019  test loss 709.2088\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000020  train loss 1792.1110\n",
      "Batch 00000020  test loss 150.8555\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000021  train loss 1154.0769\n",
      "Batch 00000021  test loss 305.8101\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000022  train loss 611.7297\n",
      "Batch 00000022  test loss 369.4409\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000023  train loss 860.7128\n",
      "Batch 00000023  test loss 745.4946\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000024  train loss 211.8060\n",
      "Batch 00000024  test loss 900.2535\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000025  train loss 1011.8244\n",
      "Batch 00000025  test loss 930.1719\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000026  train loss 702.3428\n",
      "Batch 00000026  test loss 412.0258\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000027  train loss 318.7416\n",
      "Batch 00000027  test loss 488.4058\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000028  train loss 506.3075\n",
      "Batch 00000028  test loss 290.4380\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000029  train loss 825.5143\n",
      "Batch 00000029  test loss 655.8306\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000030  train loss 485.9873\n",
      "Batch 00000030  test loss 1990.4133\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000031  train loss 1901.9785\n",
      "Batch 00000031  test loss 407.0020\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000032  train loss 645.7144\n",
      "Batch 00000032  test loss 1291.6239\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000033  train loss 315.6541\n",
      "Batch 00000033  test loss 328.4930\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000034  train loss 1210.9594\n",
      "Batch 00000034  test loss 1341.9741\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000035  train loss 533.3652\n",
      "Batch 00000035  test loss 381.2978\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000036  train loss 798.5599\n",
      "Batch 00000036  test loss 529.5908\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000037  train loss 1732.8965\n",
      "Batch 00000037  test loss 1441.6461\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000038  train loss 1223.4438\n",
      "Batch 00000038  test loss 370.8774\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000039  train loss 1139.3118\n",
      "Batch 00000039  test loss 1021.3953\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000040  train loss 491.1790\n",
      "Batch 00000040  test loss 680.4213\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000041  train loss 791.1111\n",
      "Batch 00000041  test loss 711.5436\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000042  train loss 1305.1973\n",
      "Batch 00000042  test loss 1267.7761\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000043  train loss 829.1307\n",
      "Batch 00000043  test loss 585.6887\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000044  train loss 422.3799\n",
      "Batch 00000044  test loss 1316.7129\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000045  train loss 700.9682\n",
      "Batch 00000045  test loss 1159.4276\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000046  train loss 869.7199\n",
      "Batch 00000046  test loss 1313.9998\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000047  train loss 671.8069\n",
      "Batch 00000047  test loss 940.1945\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000048  train loss 1044.0150\n",
      "Batch 00000048  test loss 1301.9359\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000049  train loss 860.5069\n",
      "Batch 00000049  test loss 239.9089\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000050  train loss 1626.9144\n",
      "Batch 00000050  test loss 952.2761\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000051  train loss 780.0586\n",
      "Batch 00000051  test loss 707.6936\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000052  train loss 388.1816\n",
      "Batch 00000052  test loss 1233.3729\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000053  train loss 351.2795\n",
      "Batch 00000053  test loss 646.6148\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000054  train loss 374.5473\n",
      "Batch 00000054  test loss 529.2734\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000055  train loss 1401.4226\n",
      "Batch 00000055  test loss 316.2193\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000056  train loss 314.9445\n",
      "Batch 00000056  test loss 883.3945\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000057  train loss 389.2275\n",
      "Batch 00000057  test loss 600.9894\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000058  train loss 889.5938\n",
      "Batch 00000058  test loss 407.4388\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000059  train loss 1582.4758\n",
      "Batch 00000059  test loss 299.4373\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000060  train loss 311.0761\n",
      "Batch 00000060  test loss 397.6651\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000061  train loss 311.7825\n",
      "Batch 00000061  test loss 654.1422\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000062  train loss 664.9393\n",
      "Batch 00000062  test loss 1861.7733\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000063  train loss 1243.3344\n",
      "Batch 00000063  test loss 338.2881\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000064  train loss 1046.5597\n",
      "Batch 00000064  test loss 1200.5922\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000065  train loss 465.6429\n",
      "Batch 00000065  test loss 531.0200\n",
      "Generating New Examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Examples Generated\n",
      "Batch 00000066  train loss 927.4522\n",
      "Batch 00000066  test loss 497.3214\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000067  train loss 602.0576\n",
      "Batch 00000067  test loss 1138.3055\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000068  train loss 476.7871\n",
      "Batch 00000068  test loss 1032.7307\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000069  train loss 1554.1077\n",
      "Batch 00000069  test loss 1143.3466\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000070  train loss 887.3333\n",
      "Batch 00000070  test loss 1353.4348\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000071  train loss 374.5355\n",
      "Batch 00000071  test loss 507.3273\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000072  train loss 770.4086\n",
      "Batch 00000072  test loss 316.2920\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000073  train loss 1688.8949\n",
      "Batch 00000073  test loss 579.1281\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000074  train loss 840.7083\n",
      "Batch 00000074  test loss 1255.8663\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000075  train loss 397.9375\n",
      "Batch 00000075  test loss 1185.1760\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000076  train loss 451.5124\n",
      "Batch 00000076  test loss 1063.5204\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000077  train loss 637.5786\n",
      "Batch 00000077  test loss 1697.2861\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000078  train loss 258.6059\n",
      "Batch 00000078  test loss 417.7097\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000079  train loss 374.1472\n",
      "Batch 00000079  test loss 371.5909\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000080  train loss 446.8386\n",
      "Batch 00000080  test loss 905.3931\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000081  train loss 1671.0197\n",
      "Batch 00000081  test loss 1226.0302\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000082  train loss 550.0959\n",
      "Batch 00000082  test loss 696.1868\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000083  train loss 686.6791\n",
      "Batch 00000083  test loss 1839.0074\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000084  train loss 642.6814\n",
      "Batch 00000084  test loss 1078.3251\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000085  train loss 846.6273\n",
      "Batch 00000085  test loss 950.4749\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000086  train loss 757.0518\n",
      "Batch 00000086  test loss 314.5726\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000087  train loss 1095.0322\n",
      "Batch 00000087  test loss 638.5704\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000088  train loss 1143.6327\n",
      "Batch 00000088  test loss 454.4965\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000089  train loss 1416.0157\n",
      "Batch 00000089  test loss 660.7704\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000090  train loss 359.7843\n",
      "Batch 00000090  test loss 1687.9086\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000091  train loss 330.7407\n",
      "Batch 00000091  test loss 1418.5874\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000092  train loss 883.0779\n",
      "Batch 00000092  test loss 489.3288\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000093  train loss 478.6616\n",
      "Batch 00000093  test loss 1262.4806\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000094  train loss 1045.9734\n",
      "Batch 00000094  test loss 473.5745\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000095  train loss 549.4954\n",
      "Batch 00000095  test loss 402.5618\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000096  train loss 411.6090\n",
      "Batch 00000096  test loss 855.4619\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000097  train loss 472.2807\n",
      "Batch 00000097  test loss 290.2596\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000098  train loss 414.7174\n",
      "Batch 00000098  test loss 422.3048\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000099  train loss 1651.6095\n",
      "Batch 00000099  test loss 1825.3867\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000100  train loss 524.6549\n",
      "Batch 00000100  test loss 392.9740\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000101  train loss 968.4932\n",
      "Batch 00000101  test loss 477.6671\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000102  train loss 732.2389\n",
      "Batch 00000102  test loss 665.9043\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000103  train loss 1523.3157\n",
      "Batch 00000103  test loss 694.5991\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000104  train loss 1181.6721\n",
      "Batch 00000104  test loss 1600.8909\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000105  train loss 399.0572\n",
      "Batch 00000105  test loss 352.0770\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000106  train loss 409.2745\n",
      "Batch 00000106  test loss 913.2498\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000107  train loss 568.1846\n",
      "Batch 00000107  test loss 1479.8881\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000108  train loss 239.0156\n",
      "Batch 00000108  test loss 903.1812\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000109  train loss 519.3141\n",
      "Batch 00000109  test loss 983.8904\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000110  train loss 657.4780\n",
      "Batch 00000110  test loss 1706.3851\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000111  train loss 2020.4441\n",
      "Batch 00000111  test loss 854.5072\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000112  train loss 1149.6787\n",
      "Batch 00000112  test loss 390.1436\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000113  train loss 304.0778\n",
      "Batch 00000113  test loss 1451.0721\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000114  train loss 402.2962\n",
      "Batch 00000114  test loss 562.3956\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000115  train loss 723.6260\n",
      "Batch 00000115  test loss 709.8206\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000116  train loss 964.3334\n",
      "Batch 00000116  test loss 307.0955\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000117  train loss 447.1276\n",
      "Batch 00000117  test loss 1355.4614\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000118  train loss 781.9832\n",
      "Batch 00000118  test loss 434.5898\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000119  train loss 657.1058\n",
      "Batch 00000119  test loss 856.8367\n",
      "Generating New Examples\n",
      "New Examples Generated\n",
      "Batch 00000120  train loss 882.6147\n",
      "Batch 00000120  test loss 254.3787\n"
     ]
    },
    {
     "ename": "PortAudioError",
     "evalue": "Error opening OutputStream: Internal PortAudio error [PaErrorCode -9986]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPortAudioError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3d94b9909e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch %.8d \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test loss %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0msd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m44100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sounddevice.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(data, samplerate, mapping, blocking, loop, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m                      \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocking\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                      \u001b[0mprime_output_buffers_using_stream_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                      **kwargs)\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sounddevice.py\u001b[0m in \u001b[0;36mstart_stream\u001b[0;34m(self, StreamClass, samplerate, channels, dtype, callback, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   2496\u001b[0m                                   \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2497\u001b[0m                                   \u001b[0mfinished_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2498\u001b[0;31m                                   **kwargs)\n\u001b[0m\u001b[1;32m   2499\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2500\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_last_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sounddevice.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback)\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \"\"\"\n\u001b[1;32m   1454\u001b[0m         _StreamBase.__init__(self, kind='output', wrap_callback='array',\n\u001b[0;32m-> 1455\u001b[0;31m                              **_remove_self(locals()))\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sounddevice.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, kind, samplerate, blocksize, device, channels, dtype, latency, extra_settings, callback, finished_callback, clip_off, dither_off, never_drop_input, prime_output_buffers_using_stream_callback, userdata, wrap_callback)\u001b[0m\n\u001b[1;32m    859\u001b[0m                                   \u001b[0msamplerate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_flags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                                   callback_ptr, userdata),\n\u001b[0;32m--> 861\u001b[0;31m                'Error opening {0}'.format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;31m# dereference PaStream** --> PaStream*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sounddevice.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(err, msg)\u001b[0m\n\u001b[1;32m   2651\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mPortAudioError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrormsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhosterror_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mPortAudioError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrormsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPortAudioError\u001b[0m: Error opening OutputStream: Internal PortAudio error [PaErrorCode -9986]"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "\n",
    "# create the TF neural net\n",
    "# some hyperparams\n",
    "training_batches = 2000\n",
    "\n",
    "n_neurons_per_part_in_h1 = 500\n",
    "n_neurons_in_h2 = 500\n",
    "n_neurons_in_h3 = 500\n",
    "n_neurons_in_h4 = 500\n",
    "n_neurons_in_h5 = 200\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "sample_rate = 44100            # 44.1k samples per channel per second\n",
    "num_samples = 15*sample_rate   # 15 seconds of data\n",
    "\n",
    "num_sounds = 1721\n",
    "\n",
    "#n_features = num_samples + (num_sounds)**2\n",
    "\n",
    "num_parts = 30\n",
    "samples_per_part = num_samples // num_parts\n",
    "\n",
    "n_features = num_samples\n",
    "n_targets = num_samples\n",
    "#############################################\n",
    "\n",
    "# basic 2 layer dense net (MLP) example adapted from\n",
    "# https://becominghuman.ai/creating-your-own-neural-network-using-tensorflow-fa8ca7cc4d0e\n",
    "\n",
    "# these placeholders serve as our input tensors\n",
    "x = tf.placeholder(tf.float32, [None, n_features], name='input')\n",
    "t = tf.placeholder(tf.float32, [None, num_sounds], name='tone_transformation')\n",
    "y = tf.placeholder(tf.float32, [None, n_targets], name='labels')\n",
    "\n",
    "# TF Variables are our neural net parameter tensors, we initialize them to random (gaussian) values in\n",
    "# Layer1. Variables are allowed to be persistent across training epochs and updatable bt TF operations\n",
    "\n",
    "W1s = []\n",
    "b1s = []\n",
    "y1s = []\n",
    "\n",
    "for i in range(2*num_parts-1):\n",
    "\n",
    "    W1s.append(tf.Variable(tf.truncated_normal([samples_per_part + num_sounds, n_neurons_per_part_in_h1],\n",
    "                                              mean=0,\n",
    "                                              stddev=1 / np.sqrt(n_features)),\n",
    "                          name=f'weights1_{i}'))\n",
    "    b1s.append(tf.Variable(tf.truncated_normal([n_neurons_per_part_in_h1],\n",
    "                                              mean=0,\n",
    "                                              stddev=1 / np.sqrt(n_features)),\n",
    "                          name=f'biases1_{i}'))\n",
    "    y1s.append(\n",
    "        tf.nn.relu(\n",
    "            tf.matmul(\n",
    "                tf.concat([tf.slice(x, [0, i*samples_per_part // 2], [-1, samples_per_part]), t], axis=1),\n",
    "                W1s[i]\n",
    "            ) + b1s[i],\n",
    "            name=f'activationLayer1_{i}'\n",
    "        )\n",
    "    )\n",
    "\n",
    "y1 = tf.concat(y1s, axis=1)\n",
    "\n",
    "# note the output tensor of the 1st layer is the activation applied to a\n",
    "# linear transform of the layer 1 parameter tensors\n",
    "# the matmul operation calculates the dot product between the tensors\n",
    "\n",
    "# network parameters(weights and biases) are set and initialized (Layer2)\n",
    "W2 = tf.Variable(tf.random_normal([(2*num_parts - 1)*n_neurons_per_part_in_h1, n_neurons_in_h2],\n",
    "                                  mean=0,\n",
    "                                  stddev=1),\n",
    "                 name='weights2')\n",
    "b2 = tf.Variable(tf.random_normal([n_neurons_in_h2], mean=0, stddev=1), name='biases2')\n",
    "# activation function(sigmoid)\n",
    "y2 = tf.nn.relu((tf.matmul(y1, W2) + b2), name='activationLayer2')\n",
    "\n",
    "# network parameters(weights and biases) are set and initialized (Layer3)\n",
    "W3 = tf.Variable(tf.random_normal([n_neurons_in_h2, n_neurons_in_h3], mean=0, stddev=1),\n",
    "                 name='weights2')\n",
    "b3 = tf.Variable(tf.random_normal([n_neurons_in_h3], mean=0, stddev=1), name='biases3')\n",
    "# activation function(sigmoid)\n",
    "y3 = tf.nn.relu((tf.matmul(y2, W3) + b3), name='activationLayer2')\n",
    "\n",
    "# network parameters(weights and biases) are set and initialized (Layer4)\n",
    "W4 = tf.Variable(tf.random_normal([n_neurons_in_h3, n_neurons_in_h4], mean=0, stddev=1),\n",
    "                 name='weights2')\n",
    "b4 = tf.Variable(tf.random_normal([n_neurons_in_h4], mean=0, stddev=1), name='biases4')\n",
    "# activation function(sigmoid)\n",
    "y4 = tf.nn.relu((tf.matmul(y3, W4) + b4), name='activationLayer2')\n",
    "\n",
    "# network parameters(weights and biases) are set and initialized (Layer5)\n",
    "W5 = tf.Variable(tf.random_normal([n_neurons_in_h4, n_neurons_in_h5], mean=0, stddev=1),\n",
    "                 name='weights2')\n",
    "b5 = tf.Variable(tf.random_normal([n_neurons_in_h5], mean=0, stddev=1), name='biases5')\n",
    "# activation function(sigmoid)\n",
    "y5 = tf.nn.relu((tf.matmul(y4, W5) + b5), name='activationLayer2')\n",
    "\n",
    "# output layer weights and biases\n",
    "Wo = tf.Variable(tf.random_normal([n_neurons_in_h5, n_targets], mean=0, stddev=1 ),\n",
    "                 name='weightsOut')\n",
    "bo = tf.Variable(tf.random_normal([n_targets], mean=0, stddev=1), name='biasesOut')\n",
    "\n",
    "# the sigmoid (binary softmax) activation is absorbed into TF's sigmoid_cross_entropy_with_logits loss\n",
    "#logits = (tf.matmul(y2, Wo) + bo)\n",
    "output = tf.nn.relu(tf.matmul(y5, Wo) + bo)\n",
    "#loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "loss = mfccs_loss(y, output)\n",
    "\n",
    "# tap a separate output that applies softmax activation to the output layer\n",
    "# for training accuracy readout\n",
    "#a = tf.nn.sigmoid(logits, name='activationOutputLayer')\n",
    "\n",
    "# optimizer used to compute gradient of loss and apply the parameter updates.\n",
    "# the train_step object returned is ran by a TF Session to train the net\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# prediction accuracy\n",
    "# compare predicted value from network with the expected value/target\n",
    "\n",
    "#correct_prediction = tf.equal(tf.round(a), y)\n",
    "# accuracy determination\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\")\n",
    "\n",
    "#############################################\n",
    "# ***NOTE global_variables_initializer() must be called before creating a tf.Session()!***\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# create a session for training and feedforward (prediction). Sessions are TF's way to run\n",
    "# feed data to placeholders and variables, obtain outputs and update neural net parameters\n",
    "with tf.Session() as sess:\n",
    "    # ***initialization of all variables... NOTE this must be done before running any further sessions!***\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # training loop over the number of epochs\n",
    "    batches = 2000\n",
    "\n",
    "    for batch in range(training_batches):\n",
    "        losses = 0\n",
    "        accs = 0\n",
    "        \n",
    "        #X, Y = load_examples(np.random.randint(200, size=10))\n",
    "        print('Generating New Examples')\n",
    "        X, T, Y = create_examples(2, song)\n",
    "        print('New Examples Generated')\n",
    "        frac = 0.5\n",
    "        train_stop = int(len(X) * frac)\n",
    "        X_train = X[:train_stop]\n",
    "        T_train = T[:train_stop]\n",
    "        Y_train = Y[:train_stop]\n",
    "        X_test = X[train_stop:]\n",
    "        T_test = T[train_stop:]\n",
    "        Y_test = Y[train_stop:]\n",
    "        \n",
    "        X_b = X_train\n",
    "        T_b = T_train\n",
    "        Y_b = Y_train\n",
    "\n",
    "        # train the network, note the dictionary of inputs and labels\n",
    "        sess.run(train_step, feed_dict={x: X_b, t: T_b, y: Y_b})\n",
    "        # feedforwad the same data and labels, but grab the accuracy and loss as outputs\n",
    "        l = sess.run([loss], feed_dict={x: X_b, t: T_b, y: Y_b})\n",
    "\n",
    "        losses = np.sum(l)\n",
    "        print(\"Batch %.8d \" % batch, \"train loss %.4f\" % losses)\n",
    "\n",
    "        # test on the holdout set\n",
    "        test_output, l = sess.run([output, loss], feed_dict={x: X_test, t: T_test, y: Y_test})\n",
    "        losses = np.sum(l)\n",
    "        print(\"Batch %.8d \" % batch, \"test loss %.4f\" % losses)\n",
    "        if (batch % 5 == 0):\n",
    "            sd.play(test_output[0], 44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
